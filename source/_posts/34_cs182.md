---
title: "[Lecture Notes] ShanghaiTech CS182 Introduction to Machine Learning"
date: 2022-09-13 15:20:41
description: Lecture notes for Introduction to Machine Learning, fall 2022
math: true
hide: true
categories: 
- Lecture Notes
tags:
- Lecture Notes
- math
---





# Lecture 1. Intro

## Major topics

- Bayesian decision theory
- Parameter estimation for generative models
- Linear discrimination
- Multilayer perceptrons
- Support vector machines
- Dimensionality reduction
- Clustering and mixture models
- Nonparametric Methods
- Deep learning models
- Ensemble learning
- Model assessment and selection



# Lecture 2. Linear Algebra





# Bayesian Decision Theory

## MLE (maximum likelihood estimation)

log likelihood
$$
L(\theta|\mathcal{X}) = \log \prod_{t=1}^N P(x_t|\theta)
$$
Let $\frac{\partial}{\partial \theta} L(\theta|X) = 0$, we can get MLE $\hat{\theta}$.



## Bayesian estimator

$$
\theta_{Bayes} = E(\theta | \mathcal{X}) = \int \theta \cdot p(\theta|x) \mathrm{d}\theta
$$

If $p(\theta|\mathcal{X})$ is normal, $\theta_{Bayes} = \theta_{MAP}$. 

当后验分布为单峰，峰值处较窄，MAP比Bayes更准确.



# MLP

### Non-linear regression:

Assume a single output 
$$
y^t = \sum_{h=1}^H v_h z_h^t + v_0
$$

$$
z_h^t = \mathrm{sigmoid}(\mathbf{w}_h^t \mathrm{x}^t)
$$

Error function over the entire training sample
$$
E(\mathbf{W,v}|\mathcal{X}) 
= \frac{1}{2} \sum_{t=1}^N E^t (\mathbf{W,v}|\mathbf{x}^t, r^t) 
= \frac{1}{2} \sum_{t=1}^N (r^t - y^t)^2
$$
Update rule for second-layer weights
$$
\Delta{v_h} = 
$$






# SVM

## Hard-margin SVM

目标: 希望 **margin** 尽可能大

离hyperplane最近的点须满足 $|w^Tx + w_0| = 1$.

------

MLE: log likelihood 
$$
    L(\theta|X) = \log \prod_{t=1}^N P(x_t|\theta)
$$
令 $\frac{\partial}{\partial \theta}L(\theta|X)=0$, 得到 Maximum likelihood estimate $\hat{\theta}$.

Bayesian estimator:

$$
    \theta_{MAP} = \arg\max_\theta p(\theta|X) = \arg\max_\theta P(X\theta) p(\theta)
$$

If $p(\theta)$ are equal, $\theta_{MAP} = \theta_{MLE} = \arg\max_\theta P(X|\theta)$.

$$
    \theta_{Bayes} = E(\theta|X) = \int \theta\cdot p(\theta|X) \mathrm{d} \theta
$$

If $p(\theta)$ is normal (gaussian), $\theta_{Bayes}=\theta_{MAP}$.


------




